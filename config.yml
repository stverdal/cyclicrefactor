llm:
  provider: none
  model: ""
  params:
    num_ctx: 4096
    temperature: 0.0

# Context management settings
context:
  # Total context window size (tokens) - should match your LLM's limit
  window_size: 4096
  # Per-file character limits (used as fallback if budget system not active)
  max_file_chars: 4000
  max_diff_chars: 6000
  # RAG context limits
  max_rag_chars: 1500

io:
  artifacts_dir: artifacts
  cyclic_folder: cyclicDepen
  prompts_dir: prompts

pipeline:
  agents_order: [describer, refactor, validator, explainer]
  max_iterations: 2
  enable:
    describer: true
    refactor: true
    validator: true
    explainer: true

prompts:
  describer: prompts/prompt_describer.txt
  refactor: prompts/prompt_refactor.txt
  validator: prompts/prompt_validator.txt
  explainer: prompts/prompt_explainer.txt

retriever:
  type: chroma
  persist_dir: cache/chroma_db
  data_dir: data/pdf
  embedding_provider: huggingface
  embedding_model: all-MiniLM-L6-v2
  collection_name: architecture_docs
  search_type: similarity
  search_kwargs:
    k: 4

validator:
  linters:
    python: "flake8"
  test_command: "pytest -q"

logging:
  level: INFO
  log_file: langCodeUnderstanding.log
