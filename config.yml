llm:
  provider: ollama
  model: "qwen2.5-coder-32768:14b"
  params:
    num_ctx: 16384
    temperature: 0.0

# Context management settings
context:
  # Total context window size (tokens) - should match your LLM's limit
  window_size: 16384
  # Per-file character limits (used as fallback if budget system not active)
  max_file_chars: 4000
  max_diff_chars: 6000
  # RAG context limits
  max_rag_chars: 1500

# Refactor agent settings
refactor:
  min_match_confidence: 0.7   # Minimum confidence to accept SEARCH match
  warn_confidence: 0.85       # Warn if below this (but still apply)
  allow_low_confidence: false # Accept matches below min_confidence?
  atomic_mode: true           # All SEARCH/REPLACE blocks must succeed
  atomic_proposal: true       # All files must succeed or all reverted
  compact_prompts: true       # Use shorter prompts for low-VRAM LLMs
  auto_compact_threshold: 8192  # Auto-enable compact if context below this
  # Sequential file mode - two-phase approach for better results
  sequential_file_mode: false   # Disabled when suggestion_mode is on
  auto_sequential_threshold: 3  # Auto-enable if cycle has >= N files
  # Suggestion mode - output human-reviewable suggestions instead of applying patches
  suggestion_mode: true         # If true, output suggestions for review (no patches applied)
  suggestion_context_lines: 7   # Lines of context around changes
  suggestion_output_format: markdown  # "markdown" or "json"
  minimal_diff_mode: true      # If true, generate only the minimal change needed
  # Line-based patching - uses line numbers instead of SEARCH/REPLACE
  line_based_patching: true     # Use line numbers for more reliable patching
  line_patch_backup: true       # Create backup before applying line patches
  # Auto-repair settings
  auto_repair_syntax: true      # Attempt to auto-repair syntax errors in LLM output
  auto_repair_min_confidence: 0.6  # Minimum confidence to accept auto-repair
  # Validation settings
  rule_based_validation: true   # Run rule-based validation (set false to skip)
  block_on_validation_failure: true  # Reject patches that fail validation
  hallucination_detection: true # Detect and warn about LLM hallucinations

io:
  artifacts_dir: artifacts
  cyclic_folder: cyclicDepen
  prompts_dir: prompts

pipeline:
  agents_order: [describer, refactor, validator, explainer]
  max_iterations: 2
  enable:
    describer: true
    refactor: true
    validator: true
    explainer: true

# Prompt templates - compact versions used when compact_prompts=true or auto-detected
prompts:
  describer: prompts/prompt_describer.txt
  describer_compact: prompts/prompt_describer_compact.txt
  refactor: prompts/prompt_refactor.txt
  refactor_compact: prompts/prompt_refactor_compact.txt
  refactor_plan: prompts/prompt_refactor_plan.txt
  refactor_plan_compact: prompts/prompt_refactor_plan_compact.txt
  refactor_file: prompts/prompt_refactor_file.txt
  refactor_file_compact: prompts/prompt_refactor_file_compact.txt
  refactor_line_patch: prompts/prompt_line_patch.txt
  refactor_simple_format: prompts/prompt_simple_format.txt
  validator: prompts/prompt_validator.txt
  validator_compact: prompts/prompt_validator_compact.txt
  explainer: prompts/prompt_explainer.txt
  explainer_compact: prompts/prompt_explainer_compact.txt

retriever:
  type: chroma
  persist_dir: cache/chroma_db
  data_dir: data/pdf
  embedding_provider: huggingface
  embedding_model: all-MiniLM-L6-v2
  collection_name: architecture_docs
  search_type: similarity
  search_kwargs:
    k: 4

validator:
  linters:
    python: "flake8"
  test_command: "pytest -q"

logging:
  level: INFO
  log_file: langCodeUnderstanding.log
  console: true
  # LLM input/output logging (for debugging and analysis)
  log_llm_io: true            # Log all LLM prompts and responses
  llm_io_log_file: llm_io.log # Separate file for LLM I/O logs
  log_llm_prompts: true       # Log full prompts sent to LLM
  log_llm_responses: true     # Log full LLM responses
  truncate_llm_logs: 0        # Max chars per log entry (0 = no truncation)
  log_llm_timestamps: true    # Include timestamps in LLM logs
